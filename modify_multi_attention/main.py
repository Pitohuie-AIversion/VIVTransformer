import torch
import yaml
from data.dataloader import get_loaders
from mymodels.transformer import TransformerFlowReconstructionModel
from training.trainer import train_model, test_model
from utils.visualization import plot_losses
import os
import matplotlib.pyplot as plt  # æ˜ç¡®å¯¼å…¥matplotlib
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’æ¨¡å¼ï¼Œé˜²æ­¢å¼¹çª—

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

# ATTENTION_TYPES = [
#     # "external", "self", "simplified_self", "muse", "ufo", "aft", "vip", "halo",
#     "se", "sk", "cbam", "bam", "eca", "danet", "psa", "shuffle", "muse", "sge", "a2", "aft",
#     "outlook", "vip", "coatnet", "halo", "polarized", "cot",
#     "residual", "s2", "crossformer", "moa", "dat", "parnet", "mobilevit", "mobilevitv2"
# ]

import os
import yaml
import torch
import matplotlib.pyplot as plt
from modify_multi_attention.data.dataloader import get_loaders
from modify_multi_attention.mymodels.transformer import TransformerFlowReconstructionModel
from modify_multi_attention.training.trainer import train_model, test_model
from modify_multi_attention.utils.visualization import plot_losses


def main():
    # è¯»å–é…ç½®æ–‡ä»¶
    with open('modify_multi_attention/configs/config.yaml', 'r', encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    device = torch.device(cfg["device"])
    print(f"Using device: {device}")

    # è¯»å– YAML é…ç½®
    ATTENTION_TYPES = cfg["attention_types"]
    vis_enabled = cfg["visualization"]["enabled"]
    parent_dir = "attention_results"
    os.makedirs(parent_dir, exist_ok=True)

    failed_attention_types = []  # è®°å½•å¤±è´¥çš„æ³¨æ„åŠ›æœºåˆ¶

    train_loader, valid_loader, test_loader = get_loaders(cfg["data"]["path"], cfg["data"]["batch_size"])

    for attn_type in ATTENTION_TYPES:
        print(f"\n=========== å½“å‰æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶: {attn_type} ===========")

        try:
            model = TransformerFlowReconstructionModel(
                input_dim=cfg["model"]["input_dim"],
                output_dim=cfg["model"]["output_dim"],
                num_heads=cfg["model"]["num_heads"],
                num_layers=cfg["model"]["num_layers"],
                d_model=cfg["model"]["d_model"],
                max_time_steps=cfg["model"]["max_time_steps"],
                attention_type=attn_type
            ).to(device)

            criterion = torch.nn.MSELoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=cfg["training"]["learning_rate"])

            # è®­ç»ƒæ¨¡å‹
            trained_model, train_loss, valid_loss, test_loss = train_model(
                model, train_loader, valid_loader, test_loader,
                criterion, optimizer, cfg["training"]["epochs"],
                device, cfg["training"]["early_stop_patience"],
                attention_type=attn_type
            )

            # åˆ›å»ºå­˜å‚¨è·¯å¾„
            result_dir = os.path.join(parent_dir, attn_type)
            os.makedirs(result_dir, exist_ok=True)

            # âœ… **ä¿å­˜è®­ç»ƒå¥½çš„æœ€ä½³æ¨¡å‹**
            best_model_path = os.path.join(result_dir, f"best_model_{attn_type}.pt")
            torch.save(trained_model.state_dict(), best_model_path)

            # âœ… **ç»˜åˆ¶æŸå¤±æ›²çº¿**
            if vis_enabled:
                plot_losses(train_loss, valid_loss, test_loss)
                loss_fig_path = os.path.join(result_dir, f"loss_curve_{attn_type}.png")
                plt.savefig(loss_fig_path)
                plt.close()

            # âœ… **æµ‹è¯•æ¨¡å‹**
            final_test_loss = test_model(
                trained_model, test_loader, criterion, device,
                attention_type=attn_type,
                parent_dir="attention_results"
            )

            # âœ… **ä¿å­˜æµ‹è¯•ç»“æœ**
            test_result_file = os.path.join(result_dir, f"test_result_{attn_type}.txt")
            with open(test_result_file, 'w') as f:
                f.write(f"Test Loss for {attn_type}: {final_test_loss}\n")

            print(f"âœ… {attn_type} è®­ç»ƒå®Œæˆï¼")

        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯ï¼Œè·³è¿‡ {attn_type} æ³¨æ„åŠ›æœºåˆ¶")
            print(f"âš ï¸ é”™è¯¯è¯¦æƒ…: {str(e)}")
            failed_attention_types.append(attn_type)

    # è®°å½•å¤±è´¥çš„æ³¨æ„åŠ›æœºåˆ¶
    if failed_attention_types:
        with open(os.path.join(parent_dir, "failed_attention_log.txt"), "w") as f:
            for attn in failed_attention_types:
                f.write(f"{attn}\n")

        print(f"\nâš ï¸ ä»¥ä¸‹æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒå¤±è´¥ï¼Œå¹¶å·²è®°å½•åœ¨ failed_attention_log.txtï¼š")
        print("\n".join(failed_attention_types))
    else:
        print("\nğŸ‰ æ‰€æœ‰æ³¨æ„åŠ›æœºåˆ¶å‡è¿è¡ŒæˆåŠŸï¼")


if __name__ == "__main__":
    main()

