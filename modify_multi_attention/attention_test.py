import torch
import yaml
import os
import matplotlib.pyplot as plt
import matplotlib
from modify_multi_attention.data.dataloader import get_loaders
from modify_multi_attention.mymodels.transformer import TransformerFlowReconstructionModel
from modify_multi_attention.training.trainer import train_model, test_model
from modify_multi_attention.utils.visualization import plot_losses

matplotlib.use('Agg')
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

def main():
    # === åŠ¨æ€è·¯å¾„ ===
    current_dir = os.path.dirname(os.path.abspath(__file__))
    yaml_path = os.path.join(current_dir, "configs", "config.yaml")
    parent_dir = os.path.join(current_dir, "attention_results")
    os.makedirs(parent_dir, exist_ok=True)
    failed_log_path = os.path.join(parent_dir, "failed_attention_log.txt")

    # === è‡ªåŠ¨å†™å…¥ä½ æŒ‡å®šçš„å¤±è´¥ attention ===
    predefined_failed_list =  [
    "psa", "sge", "aft", "outlook",
    "vip", "coatnet", "halo", "residual",
    "crossformer", "moa", "dat", "mobilevit", "mobilevitv2"
]

    with open(failed_log_path, "w") as f:
        for attn in predefined_failed_list:
            f.write(f"{attn}\n")
    print(f"âœ… å·²è‡ªåŠ¨é‡å†™ failed_attention_log.txtï¼Œå…± {len(predefined_failed_list)} ä¸ª attention")

    # === è¯»å– config.yaml ===
    try:
        with open(yaml_path, 'r', encoding="utf-8") as f:
            cfg = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {yaml_path}")
        return

    device = torch.device(cfg["device"])
    print(f"ğŸ–¥ï¸ Using device: {device}")

    # === è¯»å– failed log ===
    ATTENTION_TYPES = []
    if os.path.exists(failed_log_path):
        print(f"\nğŸ“„ è¯»å–å¤±è´¥è®°å½•: {failed_log_path}")
        with open(failed_log_path, "r") as f:
            for line in f:
                tokens = line.strip().split()  # ç©ºæ ¼æˆ–æ¢è¡Œ
                ATTENTION_TYPES.extend(tokens)
        print(f"ğŸ” æœ¬æ¬¡å°†é‡æ–°æµ‹è¯•ä»¥ä¸‹ attention: {ATTENTION_TYPES}")
    else:
        print("\nâš ï¸ æ²¡æœ‰æ£€æµ‹åˆ° failed_attention_log.txtï¼Œé€€å‡º")
        return

    vis_enabled = cfg["visualization"]["enabled"]
    failed_attention_types = []

    # === åŠ è½½æ•°æ® ===
    train_loader, valid_loader, test_loader = get_loaders(cfg["data"]["path"], cfg["data"]["batch_size"])

    # === éå† attention æµ‹è¯• ===
    for attn_type in ATTENTION_TYPES:
        print(f"\n=========== æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶: {attn_type} ===========")

        try:
            model = TransformerFlowReconstructionModel(
                input_dim=cfg["model"]["input_dim"],
                output_dim=cfg["model"]["output_dim"],
                num_heads=cfg["model"]["num_heads"],
                num_layers=cfg["model"]["num_layers"],
                d_model=cfg["model"]["d_model"],
                max_time_steps=cfg["model"]["max_time_steps"],
                attention_type=attn_type
            ).to(device)

            criterion = torch.nn.MSELoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=cfg["training"]["learning_rate"])

            # === è®­ç»ƒ ===
            trained_model, train_loss, valid_loss, test_loss = train_model(
                model, train_loader, valid_loader, test_loader,
                criterion, optimizer, cfg["training"]["epochs"],
                device, cfg["training"]["early_stop_patience"],
                attention_type=attn_type
            )

            # === ä¿å­˜æ¨¡å‹ ===
            result_dir = os.path.join(parent_dir, attn_type)
            os.makedirs(result_dir, exist_ok=True)
            torch.save(trained_model.state_dict(), os.path.join(result_dir, f"best_model_{attn_type}.pt"))

            # === æŸå¤±æ›²çº¿ ===
            if vis_enabled:
                plot_losses(train_loss, valid_loss, test_loss)
                plt.savefig(os.path.join(result_dir, f"loss_curve_{attn_type}.png"))
                plt.close()

            # === æµ‹è¯• ===
            final_test_loss = test_model(
                trained_model, test_loader, criterion, device,
                attention_type=attn_type, parent_dir=parent_dir
            )

            # === ä¿å­˜ test ç»“æœ ===
            with open(os.path.join(result_dir, f"test_result_{attn_type}.txt"), 'w') as f:
                f.write(f"Test Loss for {attn_type}: {final_test_loss}\n")

            print(f"âœ… {attn_type} æµ‹è¯•å®Œæˆï¼")

        except Exception as e:
            print(f"âŒ {attn_type} å‡ºç°é”™è¯¯ï¼Œè·³è¿‡")
            print(f"âš ï¸ é”™è¯¯è¯¦æƒ…: {str(e)}")
            failed_attention_types.append(attn_type)

    # === æ›´æ–°å¤±è´¥è®°å½• ===
    if failed_attention_types:
        with open(failed_log_path, "w") as f:
            for attn in failed_attention_types:
                f.write(f"{attn}\n")
        print(f"\nâš ï¸ ä»¥ä¸‹ Attention æœºåˆ¶ä»å¤±è´¥ï¼Œlog å·²æ›´æ–°ï¼š")
        print("\n".join(failed_attention_types))
    else:
        print("\nğŸ‰ æœ¬æ¬¡æ‰€æœ‰ Attention å‡å·²æˆåŠŸ")
        if os.path.exists(failed_log_path):
            os.remove(failed_log_path)
            print("âœ… å·²æ¸…é™¤ failed_attention_log.txt")

if __name__ == "__main__":
    main()
